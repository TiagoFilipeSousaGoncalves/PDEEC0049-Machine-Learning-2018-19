{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting data....\n",
      "Elements per class in original data:  \n",
      "Class 0:  73 \n",
      "Class 1:  137\n",
      "Elements per cluster in predictions:  \n",
      "Cluster 0:  138 \n",
      "Cluster 1:  72\n",
      "According to the results, one can say that K-Means algorithm, with K=2, was able to correctly predict the clusters of our data.\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1\n",
    "#a) \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Load Data\n",
    "data = np.genfromtxt('heightWeightData.txt', delimiter=\",\")\n",
    "\n",
    "#Split Data into Labels and Features\n",
    "#Convert labels into 0-1\n",
    "labels = data[:,0] - 1\n",
    "#Features\n",
    "features = data[:, 1:3]\n",
    "\n",
    "#Create Classifier K-Means, with K=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "#Obtain clusters with k-means classifier\n",
    "print(\"Fitting data....\")\n",
    "kmeans.fit(features)\n",
    "#Make predictions on our data\n",
    "#Predictions\n",
    "y_hat = np.array(kmeans.predict(features))\n",
    "#Compare predictions with ground truth\n",
    "#Count elements of class 0 in labels\n",
    "class_0 = 0\n",
    "for label in labels:\n",
    "    if label==0:\n",
    "        class_0+=1\n",
    "\n",
    "#Count elements of class 0 in labels\n",
    "class_1 = 0\n",
    "for label in labels:\n",
    "    if label==1:\n",
    "        class_1+=1\n",
    "\n",
    "#Count elements of class 0 in labels\n",
    "pred_0 = 0\n",
    "for label in y_hat:\n",
    "    if label==0:\n",
    "        pred_0+=1\n",
    "\n",
    "#Count elements of class 0 in labels\n",
    "pred_1 = 0\n",
    "for label in y_hat:\n",
    "    if label==1:\n",
    "        pred_1+=1\n",
    "        \n",
    "#Number of elements per class\n",
    "print(\"Elements per class in original data: \", \"\\nClass 0: \", class_0, \"\\nClass 1: \", class_1)\n",
    "print(\"Elements per cluster in predictions: \", \"\\nCluster 0: \", pred_0, \"\\nCluster 1: \", pred_1)\n",
    "print(\"According to the results, one can say that K-Means algorithm, with K=2, was able to correctly predict the clusters of our data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b)\n",
    "#New approach from: https://github.com/alexkimxyz/XMeans/blob/master/xmeans.py\n",
    "\n",
    "\"\"\"\n",
    "Implementation of XMeans algorithm based on\n",
    "Pelleg, Dan, and Andrew W. Moore. \"X-means: Extending K-means with Efficient Estimation of the Number of Clusters.\"\n",
    "ICML. Vol. 1. 2000.\n",
    "https://www.cs.cmu.edu/~dpelleg/download/xmeans.pdf\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "EPS = np.finfo(float).eps\n",
    "\n",
    "\n",
    "def loglikelihood(R, R_n, variance, M, K):\n",
    "    \"\"\"\n",
    "    See Pelleg's and Moore's for more details.\n",
    "    :param R: (int) size of cluster\n",
    "    :param R_n: (int) size of cluster/subcluster\n",
    "    :param variance: (float) maximum likelihood estimate of variance under spherical Gaussian assumption\n",
    "    :param M: (float) number of features (dimensionality of the data)\n",
    "    :param K: (float) number of clusters for which loglikelihood is calculated\n",
    "    :return: (float) loglikelihood value\n",
    "    \"\"\"\n",
    "    if 0 <= variance <= EPS:\n",
    "        res = 0\n",
    "    else:\n",
    "        res = R_n * (np.log(R_n) - np.log(R) - 0.5 * (np.log(2 * np.pi) + M * np.log(variance) + 1)) + 0.5 * K\n",
    "        if res == np.inf:\n",
    "            res = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_additonal_k_split(K, X, clst_labels, clst_centers, n_features, K_sub, k_means_args):\n",
    "    bic_before_split = np.zeros(K)\n",
    "    bic_after_split = np.zeros(K)\n",
    "    clst_n_params = n_features + 1\n",
    "    add_k = 0\n",
    "    for clst_index in range(K):\n",
    "        clst_points = X[clst_labels == clst_index]\n",
    "        clst_size = clst_points.shape[0]\n",
    "        if clst_size <= K_sub:\n",
    "            # skip this cluster if it is too small\n",
    "            # i.e. cannot be split into more clusters\n",
    "            continue\n",
    "        clst_variance = np.sum((clst_points - clst_centers[clst_index]) ** 2) / float(clst_size - 1)\n",
    "        bic_before_split[clst_index] = loglikelihood(clst_size, clst_size, clst_variance, n_features,\n",
    "                                                     1) - clst_n_params / 2.0 * np.log(clst_size)\n",
    "        kmeans_subclst = KMeans(n_clusters=K_sub, **k_means_args).fit(clst_points)\n",
    "        subclst_labels = kmeans_subclst.labels_\n",
    "        subclst_centers = kmeans_subclst.cluster_centers_\n",
    "        log_likelihood = 0\n",
    "        for subclst_index in range(K_sub):\n",
    "            subclst_points = clst_points[subclst_labels == subclst_index]\n",
    "            subclst_size = subclst_points.shape[0]\n",
    "            if subclst_size <= K_sub:\n",
    "                # skip this subclst_size if it is too small\n",
    "                # i.e. won't be splittable into more clusters on the next iteration\n",
    "                continue\n",
    "            subclst_variance = np.sum((subclst_points - subclst_centers[subclst_index]) ** 2) / float(\n",
    "                subclst_size - K_sub)\n",
    "            log_likelihood = log_likelihood + loglikelihood(clst_size, subclst_size, subclst_variance, n_features,\n",
    "                                                            K_sub)\n",
    "        subclst_n_params = K_sub * clst_n_params\n",
    "        bic_after_split[clst_index] = log_likelihood - subclst_n_params / 2.0 * np.log(clst_size)\n",
    "        # Count number of additional clusters that need to be created based on BIC comparison\n",
    "        if bic_before_split[clst_index] < bic_after_split[clst_index]:\n",
    "            add_k += 1\n",
    "    return add_k\n",
    "\n",
    "\n",
    "class XMeans(KMeans):\n",
    "    def __init__(self, kmax=50, max_iter=1000, **k_means_args):\n",
    "        \"\"\"\n",
    "        :param kmax: maximum number of clusters that XMeans can divide the data in\n",
    "        :param max_iter: maximum number of iterations for the `while` loop (hard limit)\n",
    "        :param k_means_args: all other parameters supported by sklearn's KMeans algo (except `n_clusters`)\n",
    "        \"\"\"\n",
    "        if 'n_clusters' in k_means_args:\n",
    "            raise Exception(\"`n_clusters` is not an accepted parameter for XMeans algorithm\")\n",
    "        if kmax < 1:\n",
    "            raise Exception(\"`kmax` cannot be less than 1\")\n",
    "        self.KMax = kmax\n",
    "        self.max_iter = max_iter\n",
    "        self.k_means_args = k_means_args\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        K = 1\n",
    "        K_sub = 2\n",
    "        K_old = K\n",
    "        n_features = np.size(X, axis=1)\n",
    "        stop_splitting = False\n",
    "        iter_num = 0\n",
    "        while not stop_splitting and iter_num < self.max_iter:\n",
    "            K_old = K\n",
    "            kmeans = KMeans(n_clusters=K, **self.k_means_args).fit(X)\n",
    "            clst_labels = kmeans.labels_\n",
    "            clst_centers = kmeans.cluster_centers_\n",
    "            # Iterate through all clusters and determine if further split is necessary\n",
    "            add_k = get_additonal_k_split(K, X, clst_labels, clst_centers, n_features, K_sub, self.k_means_args)\n",
    "            K += add_k\n",
    "            # stop splitting clusters when BIC stopped increasing or if max number of clusters in reached\n",
    "            stop_splitting = K_old == K or K >= self.KMax\n",
    "            iter_num = iter_num + 1\n",
    "        # Run vanilla KMeans with the number of clusters determined above\n",
    "        kmeans = KMeans(n_clusters=K_old, **self.k_means_args).fit(X)\n",
    "        self.labels_ = kmeans.labels_\n",
    "        self.cluster_centers_ = kmeans.cluster_centers_\n",
    "        self.inertia_ = kmeans.inertia_\n",
    "        self.n_clusters = K_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting data...\n",
      "Predicted labels: \n",
      " [0 0 0 0 0 0 1 0 2 0 0 2 2 0 0 0 0 2 0 2 0 2 2 2 2 0 0 0 0 0 2 0 0 0 1 0 0\n",
      " 0 1 0 0 2 0 0 2 2 0 2 0 0 1 0 0 1 2 1 2 0 0 0 2 0 1 0 0 0 0 2 2 0 0 1 2 2\n",
      " 0 0 2 0 2 0 0 2 0 0 0 2 0 2 0 0 0 2 0 0 0 0 0 0 2 1 1 2 2 0 2 2 2 0 0 0 0\n",
      " 2 0 0 0 0 2 1 2 0 0 0 0 0 0 2 2 2 0 2 2 2 2 0 2 0 2 0 0 0 0 2 0 0 2 0 0 0\n",
      " 0 0 2 2 2 0 0 1 0 0 2 2 2 0 2 0 0 0 0 2 0 2 0 0 0 2 0 0 0 0 0 0 2 2 0 0 1\n",
      " 0 0 0 2 0 0 2 0 0 0 0 2 0 0 2 0 0 2 0 2 2 2 2 0 2]\n",
      "Predicted cluster centers: \n",
      " [[163.83        57.36255469]\n",
      " [188.15538462 106.31607692]\n",
      " [181.37072464  74.63976812]]\n",
      "Predicted number of clusters:  3\n"
     ]
    }
   ],
   "source": [
    "#Create X-Means Instance\n",
    "x_means = XMeans(kmax=50, max_iter=1000)\n",
    "print(\"Fitting data...\")\n",
    "x_means.fit(features)\n",
    "print(\"Predicted labels: \\n\", x_means.labels_)\n",
    "print(\"Predicted cluster centers: \\n\", x_means.cluster_centers_)\n",
    "print(\"Predicted number of clusters: \", x_means.n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Another approach that uses pyclustering library, that is based on another article.\n",
    "#In case of usage of pyclustering\n",
    "#!pip install pyclustering\n",
    "\n",
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.cluster.xmeans import xmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.utils import read_sample\n",
    "from pyclustering.samples.definitions import SIMPLE_SAMPLES\n",
    "# Read sample 'simple3' from file.\n",
    "sample = features\n",
    "# Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will\n",
    "# start analysis.\n",
    "amount_initial_centers = 2\n",
    "initial_centers = kmeans_plusplus_initializer(sample, amount_initial_centers).initialize()\n",
    "# Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum\n",
    "# number of clusters that can be allocated is 20.\n",
    "xmeans_instance = xmeans(sample, initial_centers, 20)\n",
    "xmeans_instance.process()\n",
    "# Extract clustering results: clusters and their centers\n",
    "clusters = xmeans_instance.get_clusters()\n",
    "centers = xmeans_instance.get_centers()\n",
    "# Visualize clustering results\n",
    "visualizer = cluster_visualizer()\n",
    "visualizer.append_clusters(clusters, sample)\n",
    "visualizer.append_cluster(centers, None, marker='*')\n",
    "visualizer.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
