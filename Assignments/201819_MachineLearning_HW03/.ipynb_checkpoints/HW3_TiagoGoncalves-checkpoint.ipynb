{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the height/weight data from the file heightWeightData.txt. The first column is the class label (1=male, 2=female), the second column is height, the third weight. Start by replacing the weight column by the product of height and weight.\n",
    "\n",
    "For the Fisher’s linear discriminant analysis as discussed in the class, send the python/matlab code and answers for the following questions:\n",
    "\n",
    "a. What’s the SB matrix?\n",
    "\n",
    "b. What’s the SW matrix?\n",
    "\n",
    "c. What’s the optimal 1d projection direction?\n",
    "\n",
    "d. Project the data in the optimal 1d projection direction. Set the decision threshold as the middle point between the \n",
    "projected means. What’s the misclassification error rate?\n",
    "\n",
    "e. What’s your height and weight? What’s the model prediction for your case (male/female)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load Data\n",
    "data = np.genfromtxt(\"heightWeightData.txt\", delimiter=\",\")\n",
    "\n",
    "#Weight is 3rd Column\n",
    "np.set_printoptions(suppress=True)\n",
    "new_data = np.zeros(data.shape)\n",
    "for i in range(int(new_data.shape[0])):\n",
    "    new_data[i, 0] = data[i, 0]\n",
    "    new_data[i, 1] = data[i, 1]\n",
    "    new_data[i, 2] = np.multiply(data[i, 1], data[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Vector for Males Class: \n",
      " [  182.01013699 14552.85501781] \n",
      "Mean Vector for Females Class: \n",
      " [ 165.28540146 9757.31728073]\n"
     ]
    }
   ],
   "source": [
    "#Implementing Fisher's Linear Discriminant Analysis\n",
    "#Let's group data first\n",
    "#Count males (=1) and females (=2)\n",
    "nr_males = 0\n",
    "nr_females = 0\n",
    "for i in range(int(new_data.shape[0])):\n",
    "    if new_data[i, 0] == 1:\n",
    "        nr_males+=1\n",
    "    elif new_data[i, 0] == 2:\n",
    "        nr_females+=1\n",
    "#print(nr_males, nr_females)\n",
    "#Concatenate Class Sizes\n",
    "class_sizes = np.array([nr_males, nr_females])\n",
    "\n",
    "#Assign Classes\n",
    "males = np.zeros([nr_males, new_data.shape[1]])\n",
    "females = np.zeros([nr_females, new_data.shape[1]])\n",
    "m_index = 0\n",
    "f_index = 0\n",
    "for index in range(int(new_data.shape[0])):\n",
    "    if new_data[index, 0] == 1:\n",
    "        males[m_index] = new_data[index]\n",
    "        m_index+=1\n",
    "    elif new_data[index, 0] == 2:\n",
    "        females[f_index] = new_data[index]\n",
    "        f_index+=1\n",
    "\n",
    "#Calculate means vector for each class\n",
    "#Drop Label Column\n",
    "f_males = males[:, 1:]\n",
    "f_females = females[:, 1:]\n",
    "#Calculate mean vector for each class\n",
    "mean_males = np.mean(a=f_males, axis=0)\n",
    "mean_females = np.mean(a=f_females, axis=0)\n",
    "\n",
    "print(\"Mean Vector for Males Class: \\n\", mean_males,\"\\nMean Vector for Females Class: \\n\", mean_females)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What’s the SB matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_B Matrix is:  [     279.71677843 22997182.18774202]\n"
     ]
    }
   ],
   "source": [
    "#Calculate Overall Mean\n",
    "overall_mean = np.mean(new_data[:, 1:], axis=0)\n",
    "#print(\"Overall mean vector is: \", overall_mean)\n",
    "\n",
    "#Let's Compute Between Class Scatter Matrix S_B\n",
    "\"According to the slides: S_B = (m2-m1)(m2-m1).T\"\n",
    "S_B = np.multiply((mean_females-mean_males), (mean_females-mean_males).T)\n",
    "print(\"S_B Matrix is: \", S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What’s the SW matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_W Matrix is:  [2.39983269e+06 1.07899323e+09]\n"
     ]
    }
   ],
   "source": [
    "#Let's Compute Within Class Scatter Matrix S_W\n",
    "#According to Slides\n",
    "#Males Class\n",
    "scatter_male = sum(np.matmul((f_males-mean_males).T, ((f_males-mean_males).T).T))\n",
    "scatter_female = sum(np.matmul((f_females-mean_females).T, ((f_females-mean_females).T).T))\n",
    "S_W = scatter_male+scatter_female\n",
    "print(\"S_W Matrix is: \", S_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What’s the optimal 1d projection direction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal 1D Projection Direction is:  [-0.00000697 -0.00000444]\n"
     ]
    }
   ],
   "source": [
    "#Optimal Projection or Matrix W\n",
    "W = (1/S_W)*(mean_females-mean_males)\n",
    "print(\"Optimal 1D Projection Direction is: \", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Project the data in the optimal 1d projection direction. Set the decision threshold as the middle point between the \n",
    "projected means. What’s the misclassification error rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated threshold is:  -0.055232916501277526\n"
     ]
    }
   ],
   "source": [
    "#Calcultate Threshold\n",
    "tot = 0\n",
    "class_means = np.array([mean_males, mean_females])\n",
    "for mean in class_means:\n",
    "    tot += np.dot(W.T, mean)\n",
    "    #print(tot)\n",
    "w0 = 0.5 * tot\n",
    "print(\"Calculated threshold is: \", w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Error\n",
    "#For each input project the point\n",
    "features = (new_data[:, 1:]).T\n",
    "labels = new_data[:,0]\n",
    "projected = np.dot(W.T, np.array(features))\n",
    "#projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign Predictions\n",
    "predictions = []\n",
    "for item in projected:\n",
    "    if item >= w0:\n",
    "        predictions.append(2)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate is:  11.904761904761903 %\n"
     ]
    }
   ],
   "source": [
    "#Check Classification\n",
    "errors = (labels != predictions)\n",
    "n_errors = sum(errors)\n",
    "\n",
    "error_rate = (n_errors/len(predictions) * 100)\n",
    "print(\"Error Rate is: \", error_rate, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What’s your height and weight? What’s the model prediction for your case (male/female)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my case I was predicted as:  Female  which is  False\n"
     ]
    }
   ],
   "source": [
    "#My case\n",
    "my_height = 164\n",
    "my_weight = 65\n",
    "my_features = np.array([my_height, my_weight*my_height])\n",
    "my_ground_truth = \"Male\"\n",
    "\n",
    "#My Prediction\n",
    "my_projection = np.dot(W.T, my_features)\n",
    "if my_projection >= w0:\n",
    "    my_pred = \"Female\"\n",
    "else:\n",
    "    my_pred = \"Male\"\n",
    "\n",
    "print(\"In my case I was predicted as: \", my_pred, \" which is \", my_ground_truth==my_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}\n",
      "[2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 1.\n",
      " 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 2. 2. 1.\n",
      " 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 1. 1. 1. 1. 2. 1.]\n",
      "Error Rate is:  11.904761904761903 %\n",
      "\n",
      "As can be seen, our solution is right!\n"
     ]
    }
   ],
   "source": [
    "#Let's use Sklearn to see if our solution is correct\n",
    "#Using sklearn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(new_data[:, 1:], labels)\n",
    "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
    "              solver='eigen', store_covariance=False, tol=0.0001)\n",
    "print(clf.get_params())\n",
    "predictions = clf.predict(new_data[:, 1:])\n",
    "print(predictions)\n",
    "errors = sum(labels!=predictions)\n",
    "error_rate = (n_errors/len(predictions) * 100)\n",
    "print(\"Error Rate is: \", error_rate, \"%\")\n",
    "print(\"\\nAs can be seen, our solution is right!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider the Logistic Regression as discussed in the class. Assume now that the cost of erring an observation from class 1 is cost1 and the cost of erring observations from class 0 is cost0. How would you modify the goal function, gradient and hessian matrix (slides 11 and 12 in week 5)?\n",
    "\n",
    "Change the code provided (or developed by you) in the class to receive as input the vector of costs. Test your code with the following script:\n",
    "\n",
    "trainC1 = mvnrnd([21 21], [1 0; 0 1], 1000);\n",
    "\n",
    "trainC0 = mvnrnd([23 23], [1 0; 0 1], 20);\n",
    "\n",
    "testC1 = mvnrnd([21 21], [1 0; 0 1], 1000);\n",
    "\n",
    "testC0 = mvnrnd([23 23], [1 0; 0 1], 1000);\n",
    "\n",
    "NA = size(trainC1,1);\n",
    "\n",
    "NB = size(trainC0,1);\n",
    "\n",
    "traindata = [trainC1 ones(NA,1); trainC2 zeros(NB,1)]; %add class label in the last column\n",
    "\n",
    "weights=logReg(traindata(:,1:end-1),traindata(:,end),[NB NA])\n",
    "\n",
    "testC1 = [ones(size(testC1,1),1) testC1]; %add virtual feature for offset\n",
    "\n",
    "testC0 = [ones(size(testC0,1),1) testC0]; %add virtual feature for offset\n",
    "\n",
    "%FINISH the script to compute the recall, precision and F1 score in the test data\n",
    "\n",
    "In this script the cost of erring in C1 is proportional to the elements in C0. Compute the precision, recall and F1 in the test data. Note: if you are unable to modify to account for costs, solve without costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's implement Logistic Regression according to the slides\n",
    "#First define sigmoid function that will give us our hipothesis\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#Define the log_likelihood\n",
    "def log_likelihood(features,weights,labels):\n",
    "    z = np.dot(features.T, weights)\n",
    "    sigmoid_probs = sigmoid(z)\n",
    "    #cost0= -np.log(1 - h)\n",
    "    #cost1= -np.log(h)*len(labels[labels==0])\n",
    "    #Cost 1 is proportional to the elements in C0\n",
    "    cost1 = len(labels[labels==0])\n",
    "    l_likell = np.sum((-np.log(sigmoid_probs)*cost1*labels) + ((-np.log(1-sigmoid_probs))*(1-labels)))\n",
    "    return l_likell\n",
    "    #return np.sum(labels *cost1 + (1 - labels)*cost0)\n",
    "\n",
    "#Functions to predict probabilities and classes\n",
    "def predict_proba(features, weights):\n",
    "    z = np.dot(features, weights)\n",
    "    proba = sigmoid(z)\n",
    "    return proba\n",
    "\n",
    "def predictions(features, weights, threshold):\n",
    "    probs = predict_proba(features, weights)\n",
    "    return probs >= threshold\n",
    "\n",
    "#Define Gradient Function to be used in training phase\n",
    "def gradient(features, labels, weights):\n",
    "    z = np.dot(features, weights)\n",
    "    sigmoid_probs = sigmoid(z)\n",
    "    #return np.array([[np.sum((y - sigmoid_probs) * x), np.sum((y - sigmoid_probs) * 1)]])\n",
    "    return np.dot(np.transpose(features), (sigmoid_probs - labels)) / labels.size\n",
    "    #return (np.dot(features.T, (sigmoid_probs-labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method\\nhttps://www.kaggle.com/anthonysegura/logistic-regression-from-scratch\\nhttps://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/\\nhttps://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\\nhttps://www.internalpointers.com/post/cost-function-logistic-regression '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logReg(features, labels, learning_rate):                                                             \n",
    "    # Initialize log_likelihood & parameters                                                                                                                                    \n",
    "    weights = np.zeros((features.shape[1], 1))\n",
    "    Δl = np.Infinity                                                                \n",
    "    l = log_likelihood(features, labels, weights)                                                                 \n",
    "    # Convergence Conditions                                                                                                                        \n",
    "    max_iterations = 300000                                                                                                                                     \n",
    "    for i in range(max_iterations):                                                                                                            \n",
    "        g = gradient(features, labels, weights)                                                      \n",
    "        weights = weights - learning_rate*g                                                                            \n",
    "        # Update the log-likelihood at each iteration                                     \n",
    "        l_new = log_likelihood(features, labels, weights)                                                                                                               \n",
    "        l = l_new                                                                \n",
    "    return weights     \n",
    "\n",
    "#Check\n",
    "\"\"\"\n",
    "https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method\n",
    "https://www.kaggle.com/anthonysegura/logistic-regression-from-scratch\n",
    "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/\n",
    "https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n",
    "https://www.internalpointers.com/post/cost-function-logistic-regression \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\mscthesis\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.46317017],\n",
       "       [-0.27702771]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read Data\n",
    "trainC1 = np.random.multivariate_normal([21, 21], [[1, 0], [0, 1]], 1000);\n",
    "trainC0 = np.random.multivariate_normal([23, 23], [[1, 0], [0, 1]], 20);\n",
    "testC1 = np.random.multivariate_normal([21 , 21], [[1, 0], [0, 1]], 1000);\n",
    "testC0 = np.random.multivariate_normal([23, 23], [[1, 0], [0, 1]], 1000);\n",
    "\n",
    "#Build Train Data and add class label in the last column\n",
    "NA = int(trainC1.shape[0]);\n",
    "NB = int(trainC0.shape[0]);\n",
    "labels_C1 = np.ones([NA, 1])\n",
    "trainC1 = np.concatenate((trainC1, labels_C1), axis=1)\n",
    "labels_C0 = np.zeros([NB, 1])\n",
    "trainC0 = np.concatenate((trainC0, labels_C0), axis=1)\n",
    "traindata = np.concatenate((trainC1, trainC0), axis=0)\n",
    "\n",
    "#Compute Weights\n",
    "weights=logReg(traindata[:, :2], traindata[:, 2:], learning_rate=0.01)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\mscthesis\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\envs\\mscthesis\\lib\\site-packages\\numpy\\core\\fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.10971194],\n",
       "       [-0.12874595],\n",
       "       [-0.1524693 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Data \n",
    "#add virtual feature for offset\n",
    "C1_virtualf = np.ones((int(testC1.shape[0]), 1))\n",
    "testC1 = np.concatenate((C1_virtualf , testC1), axis=1);\n",
    "C1test_labels = np.ones((int(testC1.shape[0]), 1))\n",
    "testC1 = np.concatenate((testC1, C1test_labels), axis=1)\n",
    "#add virtual feature for offset\n",
    "C0_virtualf = np.ones((int(testC0.shape[0]), 1))\n",
    "testC0 = np.concatenate((C0_virtualf, testC0), axis=1);\n",
    "C0test_labels = np.zeros((int(testC0.shape[0]), 1))\n",
    "testC0 = np.concatenate((testC0, C0test_labels), axis=1)\n",
    "testdata = np.concatenate((testC1, testC0), axis=0)\n",
    "\n",
    "#FINISH the script to compute the recall, precision and F1 score in the test data\n",
    "weights = logReg(testdata[:, :3], testdata[:, 3:], learning_rate=0.001)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9632183908045977\n",
      "Recall:  0.838\n",
      "F1:  0.8962566844919786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "label_pred = predictions(testdata[:, :3], weights, 0.5)\n",
    "label_pred = label_pred.astype(int)\n",
    "labels = testdata[:, 3:]\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(labels, label_pred).ravel()\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/(tp+fn)\n",
    "f1=2*((precision*recall)/(precision+recall))\n",
    "print('Precision: ',precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1: ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.925777331995988\n",
      "Recall:  0.923\n",
      "F1:  0.9243865798698047\n"
     ]
    }
   ],
   "source": [
    "#Let's check with sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='newton-cg', multi_class='ovr').fit(testdata[:, :3], testdata[:, 3:].ravel())\n",
    "label_pred = clf.predict(testdata[:, :3])\n",
    "tn, fp, fn, tp = confusion_matrix(testdata[:, 3:].ravel(), label_pred).ravel()\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/(tp+fn)\n",
    "f1=2*((precision*recall)/(precision+recall))\n",
    "print('Precision: ',precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1: ',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Several phenomena and concepts in real life applications are represented by\n",
    "angular data or, as is referred in the literature, directional data. Assume the\n",
    "directional variables are encoded as a periodic value in the range [0, 2π].\n",
    "Assume a two-class (y0 and y1), one dimensional classification task over a directional\n",
    "variable x, with equal a priori class probabilities.\n",
    "\n",
    "a) If the class-conditional densities are defined as p(x|y0)= e2cos(x-1)/(2 π 2.2796)\n",
    "and p(x|y1)= e3cos(x+0.9)/(2 π 4.8808), what’s the decision at x=0?\n",
    "\n",
    "b) If the class-conditional densities are defined as p(x|y0)= e2cos(x-1)/(2 π 2.2796)\n",
    "and p(x|y1)= e3cos(x-1)/(2 π 4.8808), for what values of x is the prediction equal\n",
    "to y0?\n",
    "\n",
    "c) Assume the more generic class-conditional densities defined as\n",
    "p(x|y0)= ek0cos(x- μ0)/(2 π I(k0)) and p(x|y1)= ek1cos(x-μ1)/(2 π I(k1)). In these\n",
    "expressions, ki and μi are constants and I(ki) is a constant that depends on ki.\n",
    "Show that the posterior probability p(y0|x) can be written as p(y0|x) =\n",
    "1/(1+ew0+ w1sin(x- ϴ) ), where w0, w1 and ϴ are parameters of the model (and\n",
    "depend on ki , μi and I(ki) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from math import exp, cos, pi\n",
    "\n",
    "#Create functions\n",
    "#p(x|y0)= e2cos(x-1)/(2 π 2.2796)\n",
    "def p_x_y0(x):\n",
    "    result = (exp(2*cos(x-1)))/(2*pi*2.2796)\n",
    "    return result\n",
    "\n",
    "#p(x|y1)= e3cos(x+0.9)/(2 π 4.8808)\n",
    "def p_x_y1(x):\n",
    "    result = (exp(3*cos(x+0.9)))/(2*pi*4.8808)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At x=0, decision is:  0.21048175536784317\n"
     ]
    }
   ],
   "source": [
    "#a)\n",
    "#Compute functions at x=0\n",
    "x0_y0 = p_x_y0(0)\n",
    "x0_y1 = p_x_y1(0)\n",
    "#print(x0_y0, x0_y1)\n",
    "\n",
    "#Decision at x=0 is equal to argmax(x0_y0, x0_y1)\n",
    "if x0_y0 > x0_y1:\n",
    "    decision = x0_y0\n",
    "else:\n",
    "    decision = x0_y1\n",
    "\n",
    "print(\"At x=0, decision is: \", decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction of x is equal to y0 for the following: \n",
      " [0.06346651825433926, 0.12693303650867852, 0.1903995547630178, 0.25386607301735703, 0.3173325912716963, 0.3807991095260356, 0.4442656277803748, 0.5077321460347141, 0.5711986642890533, 0.6346651825433925, 0.6981317007977318, 0.7615982190520711, 0.8250647373064104, 0.8885312555607496, 0.9519977738150889, 1.0154642920694281, 1.0789308103237674, 1.1423973285781066, 1.2058638468324459, 1.269330365086785, 1.3327968833411243, 1.3962634015954636, 1.4597299198498028, 1.5231964381041423, 1.5866629563584815, 1.6501294746128208, 1.71359599286716, 1.7770625111214993, 1.8405290293758385, 1.9039955476301778, 1.967462065884517, 2.0309285841388562, 2.0943951023931957, 2.1578616206475347, 2.221328138901874, 2.284794657156213, 2.3482611754105527, 2.4117276936648917, 2.475194211919231, 2.53866073017357, 2.6021272484279097, 2.6655937666822487, 2.729060284936588, 2.792526803190927, 2.8559933214452666, 2.9194598396996057, 2.982926357953945, 3.0463928762082846, 3.1098593944626236, 3.173325912716963, 3.236792430971302, 3.3002589492256416, 3.3637254674799806, 3.42719198573432, 3.490658503988659]\n"
     ]
    }
   ],
   "source": [
    "#b\n",
    "points = np.linspace(0, (2*pi), num=100)\n",
    "#New p(x|y1)= e3cos(x-1)/(2 π 4.8808) funtion\n",
    "def new_p_x_y1(x): \n",
    "    result = (exp(3*cos(x-1)))/(2*p*4.8808)\n",
    "    return result\n",
    "\n",
    "#Compute values\n",
    "x_y0 = []\n",
    "for i in points:\n",
    "    x_y0.append(p_x_y0(i))\n",
    "\n",
    "x_y1 = []\n",
    "for i in points:\n",
    "    x_y1.append(p_x_y1(i))\n",
    "\n",
    "results = []\n",
    "for i in range(len(points)):\n",
    "    if x_y0[i] > x_y1[i]:\n",
    "        results.append(points[i])\n",
    "\n",
    "\n",
    "print(\"The prediction of x is equal to y0 for the following: \\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
